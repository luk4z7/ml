{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "# preparing the dataset\n",
    "work_dir = '' # give your correct directory\n",
    "image_names = sorted(os.listdir(os.path.join(work_dir, '../datasets/train/')))\n",
    "\n",
    "def copy_files(prefix_str, range_start, range_end, target_dir):\n",
    "    image_paths = [os.path.join(work_dir, '../datasets/train/', prefix_str + '.' + str(i) + '.jpg') \n",
    "                   for i in range (range_start, range_end)]\n",
    "    dest_dir = os.path.join(work_dir, 'data', target_dir, prefix_str)\n",
    "    os.makedirs(dest_dir)\n",
    "    for image_path in image_paths:\n",
    "        shutil.copy(image_path, dest_dir)\n",
    "        \n",
    "if not os.path.isdir('data/train/dog'):\n",
    "    # 1.000 training example for each class and 400 validations examples for each class.\n",
    "    copy_files('dog', 0, 1000, 'train')\n",
    "    copy_files('cat', 0, 1000, 'train')\n",
    "    copy_files('dog', 1000, 1400, 'test')\n",
    "    copy_files('cat', 1000, 1400, 'test')\n",
    "\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:3086: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      "('Train data loss:', 0.023053044408464726)\n",
      "('Train data accuracy:', 0.99263333333333337)\n",
      "('Test data loss:', 0.036594781228364445)\n",
      "('Test data accuracy:', 0.98829999999999996)\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# 3_mnist_keras.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "no_classes = 10\n",
    "epochs = 2\n",
    "image_height, image_width = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], image_height, image_width, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
    "input_shape = (image_height, image_width, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, no_classes)\n",
    "\n",
    "\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test, y_test))\n",
    "train_loss, train_accuracy = simple_cnn_model.evaluate(\n",
    "    x_train, y_train, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = simple_cnn_model.evaluate(\n",
    "    x_test, y_test, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:3086: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      " 1/10 [==>...........................] 1/10 [==>...........................] - ETA: 17:41:41 - loss: 0.6949 - acc: 0.5050"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "work_dir = ''\n",
    "\n",
    "image_height, image_width = 150, 150\n",
    "train_dir = os.path.join(work_dir, './data/train')\n",
    "test_dir = os.path.join(work_dir, './data/test')\n",
    "no_classes = 2\n",
    "no_validation = 800\n",
    "epochs = 2\n",
    "batch_size = 200\n",
    "no_train = 2000\n",
    "no_test = 800\n",
    "input_shape = (image_height, image_width, 3)\n",
    "epoch_steps = no_train // batch_size\n",
    "test_steps = no_test // batch_size\n",
    "\n",
    "\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "generator_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "generator_test = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_images = generator_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "test_images = generator_test.flow_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "simple_cnn_model.fit_generator(\n",
    "    train_images,\n",
    "    steps_per_epoch=epoch_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_images,\n",
    "    validation_steps=test_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset\n",
    "batch_size = 128\n",
    "no_classes = 10\n",
    "epochs = 2\n",
    "image_height, image_width = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# load the data from disk to memory using the keras utilities:\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# reshape the vector into an image format, and define the input dimension for the\n",
    "# convolution using the code given.\n",
    "x_train = x_train.reshape(x_train.shape[0], image_height, image_width, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
    "input_shape = (image_height, image_width, 1)\n",
    "\n",
    "# convert the data type to float as follows:\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# normalize the data by subtracting the mean of the data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert the categorical labels to one-shot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test, no_classes)\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:3086: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /var/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),                                                                                                                                             \n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# Load the training data with the training parameters and fit the model:\n",
    "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test, y_test))\n",
    "\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train data loss:', 0.017581047375519604)\n",
      "('Train data accuracy:', 0.99455000000000005)\n",
      "('Test data loss:', 0.034126561679522271)\n",
      "('Test data accuracy:', 0.98919999999999997)\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = simple_cnn_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = simple_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n",
    "\n",
    "simple_cnn_model.save('simple_cnn_model.h5')\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train\n",
      "./data/test\n",
      "1.5.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              18875392  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 18,960,138\n",
      "Trainable params: 18,960,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('simple_cnn_model.h5')\n",
    "\n",
    "image_height, image_width = 150, 150\n",
    "train_dir = os.path.join(work_dir, './data/train')\n",
    "test_dir = os.path.join(work_dir, './data/test')\n",
    "no_classes = 2\n",
    "no_validation = 800\n",
    "epochs = 2\n",
    "batch_size = 200\n",
    "no_train = 2000\n",
    "no_test = 800\n",
    "input_shape = (image_height, image_width, 3)\n",
    "epoch_steps = no_train // batch_size\n",
    "test_steps = no_test // batch_size\n",
    "\n",
    "print train_dir\n",
    "print test_dir\n",
    "\n",
    "print tf.__version__\n",
    "\n",
    "simple_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have shape (28, 28, 1) but got array with shape (150, 150, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-09c1bf7245a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     validation_steps=test_steps)\n\u001b[0m",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1427\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/root/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have shape (28, 28, 1) but got array with shape (150, 150, 3)"
     ]
    }
   ],
   "source": [
    "generator_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "generator_test  = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_images = generator_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "test_images = generator_test.flow_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "    train_images,\n",
    "    steps_per_epoch=epoch_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_images,\n",
    "    validation_steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
