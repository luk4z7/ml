{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "WEIGHTS_DIR = './weights'\n",
    "ORIGINAL_TRAIN_DIR = os.path.join('../datasets/poles_lem_vdc', 'train')\n",
    "REDUCED_TRAIN_DIR = os.path.join('../datasets/poles_lem_vdc', 'train_reduced')\n",
    "REDUCED_VAL_DIR = os.path.join('../datasets/poles_lem_vdc', 'val_reduced')\n",
    "\n",
    "LEM = os.path.join('../datasets/poles_lem_vdc', 'lem')\n",
    "VCA = os.path.join('../datasets/poles_lem_vdc', 'vca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting...\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "if not os.path.isdir(WEIGHTS_DIR):\n",
    "    os.makedirs(WEIGHTS_DIR)\n",
    "\n",
    "if not os.path.isdir(REDUCED_TRAIN_DIR):\n",
    "    os.makedirs(REDUCED_TRAIN_DIR)\n",
    "\n",
    "if not os.path.isdir(REDUCED_VAL_DIR):\n",
    "    os.makedirs(REDUCED_VAL_DIR)\n",
    "        \n",
    "N_train = 1000\n",
    "N_val = 400\n",
    "\n",
    "print 'waiting...'\n",
    "\n",
    "if os.path.isdir(REDUCED_TRAIN_DIR):\n",
    "        # if not exists, create folder \"luminous\" into the:\n",
    "        if not os.path.isdir(REDUCED_TRAIN_DIR + \"/luminous\"):\n",
    "            os.makedirs(REDUCED_TRAIN_DIR + \"/luminous\")\n",
    "            \n",
    "        # if not exists, create folder \"luminous\" into the:\n",
    "        if not os.path.isdir(REDUCED_VAL_DIR + \"/luminous\"):\n",
    "            os.makedirs(REDUCED_VAL_DIR + \"/luminous\")\n",
    "        \n",
    "        dst_train_dir = os.path.join(REDUCED_TRAIN_DIR, 'luminous')\n",
    "        dst_val_dir = os.path.join(REDUCED_VAL_DIR, 'luminous')\n",
    "        \n",
    "        # clear files into folder:\n",
    "        files = glob.glob(dst_train_dir + \"/*\")\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "        \n",
    "        # clear files into folder:          \n",
    "        files = glob.glob(dst_val_dir + \"/*\")\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "            \n",
    "        # Created array/slice for manager dataset.\n",
    "        filenameLEM = [];\n",
    "        filenameVCA = [];\n",
    "        \n",
    "        # Get all names of files and put on into the \"filenameLEM\" and \"filenameVCA\"\n",
    "        for folder, subs, files in os.walk(LEM):\n",
    "            for basename in files:\n",
    "                filenameLEM.append(os.path.join(folder, basename))\n",
    "                \n",
    "        for folder, subs, files in os.walk(VCA):\n",
    "            for basename in files:\n",
    "                filenameVCA.append(os.path.join(folder, basename))\n",
    "        \n",
    "        # Copy files for new folders.\n",
    "        for f in filenameLEM[:N_train]:\n",
    "            shutil.copy(f, dst_train_dir)\n",
    "\n",
    "        for f in filenameVCA[N_train:N_train + N_val]:\n",
    "            shutil.copy(f, dst_val_dir)\n",
    "         \n",
    "print 'finish!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add a binary classification layer (sigmoid)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "    #matthews_correlation\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.optimizers.Nadam(lr=1e-6, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy',f1_score])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adadelta',\n",
    "#               metrics=['accuracy', 'f1score', 'precision', 'recall'])\n",
    "\n",
    "\n",
    "#you can use it like this\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= \"adam\",\n",
    "              metrics=[mcor,recall, f1])\n",
    "\n",
    "# model.compile(optimizer=SGD(lr=0.00001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_reduced\n",
      "Found 992 images belonging to 1 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "height, width = 224, 224\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45.,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "print REDUCED_TRAIN_DIR\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        REDUCED_TRAIN_DIR,\n",
    "        target_size=(height, width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        REDUCED_VAL_DIR,\n",
    "        target_size=(height, width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tb_callback = TensorBoard(log_dir='tb_log')\n",
    "\n",
    "training = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size,\n",
    "        callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('trained_model.h5', custom_objects={'f1_score': f1_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferência em uma imagem\n",
    "from keras.preprocessing import image\n",
    "from skimage.io import imshow\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img('data/train/IrbKd2cK8QU1lDhfnxq9.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "print preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
